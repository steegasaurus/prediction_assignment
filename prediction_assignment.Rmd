---
title: "Predicting Action from Wearable Device Data"
author: "Steeg Pierce"
date: "6/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(caret)
library(gbm)
library(randomForest)
library(rpart)
```

```{r loadData, include = FALSE}
data <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv')
quiz <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv')
```

## Summary

We will be looking at data collected from wearable devices such as smart watches during bicep curls and looking to classify it into different degrees of proper form based on the levels given in the data. I have already loaded the data with a call to read.csv() for a training set and the quiz data. With the training set, we will create a machine learning algorithm that attemps to predict the "correctness" of the exercise.

## Exploring the Data

By looking at the structure of the quiz dataframe, we can see that there are 160 variables and 100 of them are made up of majority NA. We can also see that of those with NA, the data they do carry are transformations of the other variables. As such, we can feel comfortable removing them from the variables with NA from the model. Additionally, the first 7 variables are related to time or user, which is not relevant to our predection. We will remove those ase well. We're left with 52 predictors.

```{r cleanData, include = FALSE}
trainClean <- data[, colSums(is.na(quiz)) == 0]
testClean <- quiz[, colSums(is.na(quiz)) == 0]
trainClean <- trainClean[, -c(1:7)]
testClean <- testClean[, -c(1:7)]
trainClean$classe <- factor(trainClean$classe)
```

## Training the Model

The instructions from the assignment tell us that the variable classifying the "correctness" is 'classe', so we will be using that as the dependent variable. A principle component analysis will reduce the number of predictors while capturing most of the variation.

```{r preproc, echo = TRUE}
set.seed(150)
inTrain <- createDataPartition(trainClean$classe, p = .6, list = FALSE)
training <- trainClean[inTrain, ]
temp <- trainClean[-inTrain, ]
set.seed(200)
inValidation <- createDataPartition(temp$classe, p = .5, list = FALSE)
validation <- temp[inValidation, ]
testing <- temp[-inValidation, ]
pre <- preProcess(training, method = 'pca', thresh = .9)
trainPCA <- predict(pre, newdata = training)
validPCA <- predict(pre, validation)
testPCA <- predict(pre, testing)
#Output number of principle components
pre$numComp
```

We can see here that the number of predictors is drastically cut down via a principle component analysis. It went from 53 to 18. This will speed up our model.

Training the model will include *k-fold cross-validation* with 3 folds. More folds may help . Additionally, given the amount of data, too many folds may become too computationally demanding. We will fit three models to the training data.

First, we'll fit a *Linear Discriminant Analysis*. This will serve as a baseline.

```{r ldaFit, echo = TRUE}
set.seed(250)
tc <- trainControl(method = 'cv', number = 3)
fitLDA <- train(classe ~ ., method = 'lda', trControl = tc, data = trainPCA)
fitLDA
```

It says no pre-processing because we've already done that with a principle component analysis previously. We can see an in-sample accuracy estimate of approximately 50%. It's certainly better than random, but you wouldn't want to rely on a model like that.

Next up, we'll give the *Generalized Boosted Model* a go.

```{r gbmFit, echo = TRUE}
set.seed(300)
fitGBM <- train(classe ~ ., method = 'gbm', trControl = tc, data = trainPCA,
                verbose = FALSE)
fitGBM
```

Accuracy is a bit better, but it's still not as good as we'd like. Still, we'll test it on the validation set to see how it does.

```{r gbmVal, echo = TRUE}
predGBM <- predict(fitGBM, validPCA)
confusionMatrix(validation$classe, predGBM)
```

Next up is the more powerful *Random Forest*.

```{r rfFit, echo = TRUE}
set.seed(350)
fitRF <- train(classe ~ ., method = 'rf', trControl = tc, data = trainPCA)
fitRF
```

Accuracy for the random forest is quite high. We will test this one the validation set as well to get an idea of the out-of-sample error.

```{r rfVal, echo = TRUE}
predRF <- predict(fitRF, validPCA)
confusionMatrix(validation$classe, predRF)
```

### Testing

Having seen that the *random forest* is the most accurate model on the validation set, we will use that on the testing set.

```{r rfTest, echo = TRUE}
testRF <- predict(fitRF, testPCA)
confusionMatrix(testing$classe, testRF)
```

Having confirmed a high degree of accuracy for the model, we can use that to predict the quiz set.

```{r quiz, echo = TRUE}
quizPCA <- predict(pre, testClean)
predict(fitRF, quizPCA)
```
