---
title: "Predicting Action from Wearable Device Data"
author: "Steeg Pierce"
date: "6/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(caret)
```

```{r loadData, include = FALSE}
data <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv')
quiz <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv')
```

## Summary

We will be looking at data collected from wearable devices such as smart watches during various exercise and looking to classify it into different degrees of proper form based on the levels given in the data. I have already loaded the data with a call to read.csv() for a training and test set of said data. With the training set, we will create a machine learning algorithm that attemps to predict the "correctness" of the action being taken.

## Exploring the Data

By looking at the structure of the dataframe, we can see that there are 160 variables and 67 of them are made up of 98% NA. We can also see that of those with NA, the data they do carry are transformations of the other variables. As such, we can feel comfortable removing them from the variables with NA from the model.

```{r cleanData, include = FALSE}
trainClean <- data[, colSums(is.na(data)) == 0]
testClean <- quiz[, colSums(is.na(data)) == 0]
```

## Training the Model

The instructions from the assignment tell us that the variable classifying the "correctness" is 'classe', so we will be using that as the dependent variable. We will attempt to reduce excess variables by preprocessing with principle components analysis. 

```{r preproc, echo = TRUE}
set.seed(150)
inTrain <- createDataPartition(trainClean$classe, p = .6, list = FALSE)
training <- trainClean[inTrain, ]
temp <- trainClean[-inTrain, ]
set.seed(200)
inValidation <- createDataPartition(temp$classe, p = .5, list = FALSE)
validation <- temp[inValidation, ]
testing <- temp[-inValidation, ]
pre <- preProcess(training, method = 'pca', thresh = .95)
trainPCA <- predict(pre, newdata = training)
training$classe <- factor(training$classe)
```

Training the model will include *k-fold cross-validation* with 10 folds and 10 repeats. Simulations imply that additionaly folds after 10 become less impactful and we want a stable model. Additionally, given the amount of data, too many folds may become too computationally demanding. We will fit three models to the training data: a random forest, a boosted tree, and a normal classification tree. We will also fit a stacked model of these three models before testing each of the four models on the validation data. We can test the accuracy of the most effective model on the testing set.

```{r prelim, echo = TRUE}
tc <- trainControl(method = 'repeatedcv', number = 10, repeats = 10)
set.seed(250)
# Random forest model
fitRF <- train(classe ~ ., method = 'rf', trControl = tc, data = trainPCA)
set.seed(300)
# Boosted tree
fitADA <- train(classe ~ ., method = 'ada', trControl = tc, data = trainPCA) #binary only
set.seed(350)
# Classification tree
fitTREE <- train(classe ~ ., method = 'rpart', trControl = tc, data = trainPCA)
```



### Validation and Testing

We can now test the different models on the validation set using the confusionMatrix() function to test each for accuracy. 



With a better idea of the comparitve success of each model, we will test the most accurate model on the testing set before we use it to predict classes in the quiz set.
